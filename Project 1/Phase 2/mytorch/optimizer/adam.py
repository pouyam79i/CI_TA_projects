from mytorch.optimizer import Optimizer

class Adam(Optimizer):
    def __init__(self):
        pass
    
    def zero_grad(self):
        pass